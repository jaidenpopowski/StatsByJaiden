---
title: "Stats By Jaiden"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE, warning=F}
## Global options
knitr::opts_chunk$set(cache = TRUE)
library(tidyverse) # data manipulation tools
library(fitzRoy) # fetch AFL data
library(gt) # table outputs
library(gtExtras) # fancy table addons
library(readxl) # reading excel files
library(ggrepel) # labeled graph points
library(devtools) # advanced use for GitHub integration
library(rvest) # web scraping
library(markdown) # creating blog
library(prettydoc) # theme
library(tweetrmd) # embed twitter

sc_stats <- readRDS("All_SC_Stats.rds")

supercoach_scoring_system <- read_excel("~/supercoach scoring system.xlsx") # SuperCoach scoring from terms and conditions

images <- readRDS("PlayerImages.rds") %>% 
  mutate(img = str_replace(img,"2022014","2023014"))

final <- fst::read.fst("Shiny/UpdatedSCStats/SCMatchesSorted")

model_plot <- function(x, xvar, yvar, xlab, title) {
  
  ysym <- rlang::ensym(yvar)
  xsym <- rlang::ensym(xvar)
  mae <- x %>% transmute(diff = abs({{xvar}}-{{yvar}}), mean = mean(diff)) %>% pull(mean) %>% unique()

  
  ggplot(x, aes(x={{xvar}},y={{yvar}})) +
    geom_point(alpha=0.2) +
    geom_abline(color = "blue") +
    labs(x=xlab,y="Actual SuperCoach Score",
         title = title,
         subtitle = paste0("R-squared (% of variation explained): ",round(100*summary(rlang::inject( lm(!!ysym ~ 1+!!xsym, data=x) ))$r.squared,1),"%","     MAE: ",round(mae,2)))+
    theme_minimal()
}
```

# The secrets of SuperCoach AFL scoring


I've always been intrigued by the **AFL SuperCoach** game. Players' scores are determined by a variety of playing statistics for each game, but we don't really have any idea what is included. *Effective disposals? Uncontested intercept marks? Gathers from hitouts?* There are so many statistical categories that some coaches think it is magic! There is also match scaling to ensure each match totals to approximately 3300 points.

Unlocking the secrets behind how SuperCoach is scored will give better insights to the community and help with player selection. Explaining the scoring will help answer how certain players achieve high averages, what stats are important to be good at, and who gets the biggest bonuses.

This is my first blog post so make sure you share if you enjoy! Let's try and reverse engineer SuperCoach scores!

![ ](background.PNG)

## What we know about scoring
From the SuperCoach T&C's in 2023 (which match previous years), we have the following scoring breakdown:
```{r,echo=F, fig.align='center'}
supercoach_scoring_system %>% 
  filter(Reference == "SC T&Cs") %>% 
  select(2,3) %>% 
  rbind(c("Gather from hitout","2 Points")) %>% 
  gt()
```

This is a good start but there are three main problems - where the analysis of scoring breaks down:

1. These stats are too complex and can't be viewed on the AFL website.
2. This isn't a comprehensive list of all stats used.
3. We haven't accounted for scaling.
I'll attempt to address these points while tracking how close we are to solving the formula.

# 1. Using the AFL website stats
Let's start with the default stats available on the AFL website. Of the SuperCoach score categories we can see:

* Effective kicks
* Effective handballs (Effective Disposals - Effective Kicks)
* Clangers
* Hard/loose ball gets (Contested Possessions - Contested Marks - Frees For)
* Goals
* Behinds
* Contested marks
* Uncontested marks (Marks - Contested marks)
* Tackles
* Free Kicks

Here is an example of these stats exclusively, via the My Stats option on the AFL website.
![AFL Website Stats Example](websiteexample.png)

Calculating scores using these stats and their respective point values we achieve this:
```{r, warning = F}
sc_stats %>% 
  rowwise() %>%
  mutate(SC_Prediction = sum(
    4 * effective_kicks,
    -4 * clangers,
    1.5 * (effective_handballs), # effective handballs
    4.5 * (contested_possessions - contested_marks - frees_for), # loose/hard ball gets
    2 * (marks-contested_marks), # uncontested marks
    6 * contested_marks,
    4 * tackles,
    4 * frees_for,
    -4 * frees_against,
    5 * hitouts_adv # hitouts to advantage
    )) %>%
  ungroup() %>%
  model_plot(SC_Prediction, Score, "SuperCoach prediction", "SuperCoach score estimation using AFL website stats")
```

I've introduced two metrics here to measure how accurate our predictions are.

* R-squared, which measures how much variation in the actual scores can be explained by our predictions.
* MAE, mean absolute error, which is the average difference between our predictions and the actual scores.

While an r-squared value of 83.1% might seem really good, it tells us that only 83% of variation in SC scores can be explained using the initial model. The MAE tells us that our predictions are 11.61 points off on average. Not very useful yet! It appears that middle scores are being underestimated and most huge scores are being overestimated. We need some more stat categories

## 2. Using advanced stats
We are missing quite a few stats that aren't directly accessible on the AFL website. I've found some stats via a variety of sources, while other stats can be directly calculated using others. Most importantly we can fill out the full list of stats available in the T&Cs! Let's take a look at what stats we have now:
```{r, echo = FALSE}
sc_stats %>% 
  transmute(
    season,
    round,
    player,
    team = playing_for,
    effective_kicks,
    ineffective_kicks = kicks - effective_kicks - clanger_kick,
    clanger_kicks = clanger_kick,
    effective_handballs,
    ineffective_handballs = handballs - effective_handballs - clanger_handball,
    clanger_handballs = clanger_handball,
    handball_receives = handball_received,
    hard_ball_gets = hard_ball_get,
    loose_ball_gets = loose_ball_get,
    goals,
    behinds,
    uncontested_marks,
    uncontested_intercept_marks = uncontested_intercept_mark,
    contested_marks,
    contested_intercept_marks = contested_intercept_mark,
    tackles,
    frees_for,
    frees_against,
    hitouts_to_advantage = hitouts_adv,
    gathers_from_hitout = gather_from_hitout,
    sc_score = Score
  ) %>% arrange(desc(sc_score)) %>% 
  mutate(across(where(is.numeric), ~ as.integer(.x))) %>% str()
```

The most exciting thing about this dataset is that we can see where players score their points and how well they do before their score is scaled. If we calculate all the scores based on these new stats we achieve a better fit:

```{r, echo=FALSE}
sc_stats %>% 
  rowwise() %>% 
  mutate(SumScore = sum(
    4*effective_kicks, -4*clangers, 1.5*effective_handballs, 1.5*handball_received, 4.5*hard_ball_get, 4.5*loose_ball_get, 8*goals, 1*behinds, 
    2*(uncontested_marks - uncontested_intercept_mark), 6*(contested_marks - contested_intercept_mark), 4*uncontested_intercept_mark,
    8*contested_intercept_mark, 4*tackles, 4*frees_for, -4*frees_against, 5*hitouts_adv, 2*gather_from_hitout, na.rm = T
  )) %>% 
  ungroup() %>% 
  model_plot(xvar = SumScore, yvar = Score, xlab = "Advanced Stats Prediction", title = "SuperCoach score vs advanced stats estimation")
```
We are almost at 90% of variation explained and the MAE dropped slightly. Still not perfect, but we are making progress! We still haven't addressed the higher scores being overestimated Scaling should hopefully help with this by bringing them back to standard.

## 3. Scaling the scores via the 3300 rule

If you weren't aware, SuperCoach AFL has a set value for total points in a game. Based on the predictions from before, the estimated number of games that hit this value is 93.7%, with the game average at 3582. If this is close to correct, then scaling our predictions will help a lot.

I'll be scaling each score linearly to 3300, as it's the best way I could do it. I've heard that stats are scaled quarterly but that also goes against the whole 'every game is equal' argument. Teams could only win one quarter and still get 4 premiership points.

Let's see how the scaling affects our scores:
```{r, echo = FALSE}
sc_stats %>% 
  rowwise() %>% 
  mutate(SumScore = sum(
    4*effective_kicks, -4*clangers, 1.5*effective_handballs, 1.5*handball_received, 4.5*hard_ball_get, 4.5*loose_ball_get, 8*goals, 1*behinds, 
    2*(uncontested_marks - uncontested_intercept_mark), 6*(contested_marks - contested_intercept_mark), 4*uncontested_intercept_mark,
    8*contested_intercept_mark, 4*tackles, 4*frees_for, -4*frees_against, 5*hitouts_adv, 2*gather_from_hitout, na.rm = T
  )) %>% 
  ungroup() %>% 
  group_by(match_id) %>% 
  mutate(Total = sum(SumScore),SumScoreScaled = 3300*SumScore/sum(SumScore,na.rm=T)) %>% 
  ungroup() %>% 
  model_plot(SumScoreScaled, Score, xlab = 'Scaled Prediction', title = "SuperCoach points vs Scaled Estimations")
```

Awesome! The MAE is now single digits and R-squared went above 90%, which isn't bad at all. So on average, the estimations are 8.27 points different to the actual scores. Let's see who has the biggest differences:
```{r, echo=F}
sc_stats %>% 
  rowwise() %>% 
  mutate(SumScore = sum(
    4*effective_kicks, -4*clangers, 1.5*effective_handballs, 1.5*handball_received, 4.5*hard_ball_get, 4.5*loose_ball_get, 8*goals, 1*behinds, 
    2*(uncontested_marks - uncontested_intercept_mark), 6*(contested_marks - contested_intercept_mark), 4*uncontested_intercept_mark,
    8*contested_intercept_mark, 4*tackles, 4*frees_for, -4*frees_against, 5*hitouts_adv, 2*gather_from_hitout, na.rm = T
  )) %>% 
  ungroup() %>% 
  group_by(match_id) %>% 
  mutate(Total = sum(SumScore),SumScoreScaled = 3300*SumScore/sum(SumScore,na.rm=T)) %>% 
  ungroup() %>% 
  mutate(diff = SumScore - Score, diffscaled = SumScoreScaled - Score, meandiff = mean(abs(diff)), meandiffscaled = mean(abs(diffscaled))) %>%
  group_by(id,player) %>% 
  summarise(
    games = n(),
    average = mean(Score),
    averagepd = mean(SumScore),
    averagescaled = mean(SumScoreScaled),
    diff = average - averagescaled,
    .groups = 'drop'
  ) %>% arrange((diff)) %>% filter(games>=10) %>% 
  slice_head(n=7) %>% 
  left_join(images, by = "id") %>% 
  transmute(img, Player = player, `Games`=games,`2YR SC Average`=round(average,1),`2YR Estimation` = round(averagescaled,1),Diff = round(abs(average-averagescaled),1)) %>% 
  gt() %>% 
  tab_header(title = md("**Most overestimated players**"),subtitle = "Home & Away matches from 2021-2022") %>% 
  cols_label(img = "") %>% 
  gt_img_rows(columns = 1, img_source = 'web',height = 80)
```

Seems like the Rucks are being overestimated the most. There must be some extra stats in the ruck department (sharked hitouts would be one) that bring down ruck scores.

```{r, echo=FALSE}
sc_stats %>% 
  rowwise() %>% 
  mutate(SumScore = sum(
    4*effective_kicks, -4*clangers, 1.5*effective_handballs, 1.5*handball_received, 4.5*hard_ball_get, 4.5*loose_ball_get, 8*goals, 1*behinds, 
    2*(uncontested_marks - uncontested_intercept_mark), 6*(contested_marks - contested_intercept_mark), 4*uncontested_intercept_mark,
    8*contested_intercept_mark, 4*tackles, 4*frees_for, -4*frees_against, 5*hitouts_adv, 2*gather_from_hitout, na.rm = T
  )) %>% 
  ungroup() %>% 
  group_by(match_id) %>% 
  mutate(Total = sum(SumScore),SumScoreScaled = 3300*SumScore/sum(SumScore,na.rm=T)) %>% 
  ungroup() %>% 
  mutate(diff = SumScore - Score, diffscaled = SumScoreScaled - Score, meandiff = mean(abs(diff)), meandiffscaled = mean(abs(diffscaled))) %>%
  group_by(id,player) %>% 
  summarise(
    games = n(),
    average = mean(Score),
    averagepd = mean(SumScore),
    averagescaled = mean(SumScoreScaled),
    diff = average - averagescaled,
    .groups = 'drop'
  ) %>% arrange(desc(diff)) %>% filter(games>=10) %>% 
  slice_head(n=7) %>% 
  left_join(images, by = "id") %>% 
  transmute(img, Player = player, `Games`=games,`2YR SC Average`=round(average,1),`2YR Estimation` = round(averagescaled,1),Diff = round(abs(average-averagescaled),1)) %>% 
  gt() %>% 
  tab_header(title = md("**Most underestimated players**"),subtitle = "Home & Away matches from 2021-2022") %>%
  cols_label(img = "") %>% 
  gt_img_rows(columns = 1, img_source = 'web',height = 80)
```

The key position players are being underestimated by the estimations. Dougal Howard and the other KPDs are elite in spoils so it is likely that they should be in there. Hawkins and Riewoldt stick out to me as players with a lot of score involvements/assists in high scoring teams.

## So where to next?

It is hard to add anything else in with direct certainty. Each year a SC scoring system article is posted, with slight inconsistencies to the T&Cs:

> "There are more than 50 statistical categories that count towards every player’s score.
>  ... long kicks are worth more than short kicks ... a sharked hitout is valued at -1.
> And you really don’t want any of your players giving away a 50m penalty – that’s a whopping 8.5 points off a player’s total."
>
> `r tufte::quote_footer('--- KFC SuperCoach beginner’s guide: How players score points, scoring system explained - Al Paton 06/02/2023 via FOXFOOTY.')`

Fortunately, last week [Rainman](https://twitter.com/ItsTheRainman/status/1625988150496751617) posted some tweets based on SuperCoach score values that originated from a Fantasy Freako tweet in 2018. Thanks Rainman!

```{r, echo=F}
tweet_screenshot(tweet_url("ItsTheRainman", "1625988150496751617"))
```
There are now more stats that I had with a definitive value:

* Score assists: 3 points
* Mark on lead: 5 points
* Gathers: 1.5 points
* Spoil: 2 points

Redoing all the data, I found that these stats helped to improve accuracy of score estimates!
```{r, echo=F}
final %>% 
  transmute(Score = SC, Estimated = SC - Other) %>% 
  model_plot(xvar = Estimated, yvar = Score, xlab = "Final Predicition using new stats", title = "SuperCoach points vs Final Estimations") +
  labs(caption  = "Using all known stats and scaling")
```

The MAE has dropped below 8 points and the R-squared remains above 90. I'll have to leave it there with what I currently have the resources for, and I'm happy with the end result. Unless anyone can give me a definitive value for long kicks...

# View all the scores for yourself!

I've created a small app for you to view each match score, player averages and score breakdowns. There are options to filter by player, by team, and check out all the stats from 2021 & 2022. View stat categories as averages, counts, as SC point value, or percentage of SC points. Enjoy!

[SUPERCOACH SCORE BREAKDOWNS APP](https://statsbyjaiden.shinyapps.io/supercoach_score_breakdowns/)

If you'd prefer a set of spreadsheets, perhaps you would prefer to download the data yourself to use in Excel or Google Sheets! Containing all the same stats, you can view every player, score and stat across the multiple tabs.

[SUPERCOACH SCORE BREAKDOWNS Spreadsheet](https://docs.google.com/spreadsheets/d/1v-eUuN4KdEjrDhXUHt5qmGTjQhu6wFlYalkD_KwVdnw/edit#gid=145272901)

Browse the scores via the link, or create a copy (Gsheets)/download (Excel) to sort and filter through the data yourself!

*Having any issues with the app? I'd recommend trying out the spreadsheets as they contain all the same data, but just without the easy filtering options.*

If you have made good use of these resources, consider supporting me on Kofi!
[Buy me a coffee](https://ko-fi.com/statsbyjaiden) to help me continue in posting more content like this.
